{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "from gensim.models import KeyedVectors\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import itertools\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, Merge\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras\n",
    "import gensim\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveObj(x, name):\n",
    "    #with file('test.txt', 'w') as outfile:\n",
    "    with open('../objects/'+ name + '.txt', 'wb') as f:\n",
    "        for slice_2d in x:\n",
    "            np.savetxt(f, slice_2d)\n",
    "\n",
    "def LoadObj(fileName, x1, x2):\n",
    "    with open('../objects/' + name + '.txt', 'rb') as f:\n",
    "        similarity_matrix = np.loadtxt(f)\n",
    "    similarity_matrix = new_data.reshape((x1,x2,x2))\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path of dataset\n",
    "TRAIN_CSV = '../dataset/train.txt'\n",
    "TEST_CSV = '../dataset/test.txt'\n",
    "EMBEDDING_FILE = '../models/GoogleNews-vectors-negative300.bin'\n",
    "MODEL_SAVING_DIR = '../models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test set\n",
    "train_df = pd.read_csv(TRAIN_CSV, sep=\"\\t\", error_bad_lines=False)\n",
    "test_df = pd.read_csv(TEST_CSV, sep=\"\\t\", error_bad_lines=False)\n",
    "\n",
    "train_df = train_df.rename(columns={'#1 String': 'sentence1', '#2 String': 'sentence2'})\n",
    "test_df = test_df.rename(columns={'#1 String': 'sentence1', '#2 String': 'sentence2'})\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "def text_to_word_list(text):\n",
    "    ''' Pre process and convert texts to a list of words '''\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Prepare embedding\n",
    "vocabulary = dict()\n",
    "inverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "\n",
    "sentence_cols = ['sentence1', 'sentence2']\n",
    "\n",
    "# Iterate over the questions only of both training and test datasets\n",
    "for dataset in [train_df, test_df]:\n",
    "    for index, row in dataset.iterrows():\n",
    "\n",
    "        # Iterate through the text of both questions of the row\n",
    "        for sentence in sentence_cols:\n",
    "\n",
    "            s2n = []  # q2n -> question numbers representation\n",
    "            for word in text_to_word_list(row[sentence]):\n",
    "\n",
    "                # Check for unwanted words\n",
    "                if word in stops and word not in word2vec.vocab:\n",
    "                    continue\n",
    "\n",
    "                if word not in vocabulary:\n",
    "                    vocabulary[word] = len(inverse_vocabulary)\n",
    "                    s2n.append(len(inverse_vocabulary))\n",
    "                    inverse_vocabulary.append(word)\n",
    "                else:\n",
    "                    s2n.append(vocabulary[word])\n",
    "\n",
    "            # Replace questions as word to question as number representation\n",
    "            dataset.set_value(index, sentence, s2n)\n",
    "            \n",
    "embedding_dim = 300\n",
    "embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  # This will be the embedding matrix\n",
    "embeddings[0] = 0  # So that the padding will be ignored\n",
    "\n",
    "# Build the embedding matrix\n",
    "for word, index in vocabulary.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embeddings[index] = word2vec.word_vec(word)\n",
    "\n",
    "del word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_df))\n",
    "print(len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determining maximum length of sentence\n",
    "max_seq_length = max(train_df.sentence1.map(lambda x: len(x)).max(),\n",
    "                     train_df.sentence2.map(lambda x: len(x)).max(),\n",
    "                     test_df.sentence1.map(lambda x: len(x)).max(),\n",
    "                     test_df.sentence2.map(lambda x: len(x)).max())\n",
    "\n",
    "# Split to train validation\n",
    "validation_size = 1000\n",
    "training_size = len(train_df) - validation_size\n",
    "\n",
    "X_train = train_df[sentence_cols]\n",
    "Y_train = train_df['Quality']\n",
    "\n",
    "# Split to dicts\n",
    "X_train = {'left': X_train.sentence1, 'right': X_train.sentence2}\n",
    "X_test = {'left': test_df.sentence1, 'right': test_df.sentence2}\n",
    "Y_test = test_df['Quality']\n",
    "\n",
    "# Convert labels to their numpy representations\n",
    "Y_train = Y_train.values\n",
    "Y_test = Y_test.values\n",
    "\n",
    "# Zero padding\n",
    "for dataset, side in itertools.product([X_train, X_test], ['left', 'right']):\n",
    "    dataset[side] = pad_sequences(dataset[side], maxlen=max_seq_length)\n",
    "\n",
    "# Make sure everything is ok\n",
    "assert X_train['left'].shape == X_train['right'].shape\n",
    "assert len(X_train['left']) == len(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determining size of dataset\n",
    "x1 = X_train['left'].shape[0]\n",
    "x2 = X_train['left'].shape[1]\n",
    "similarity_matrix_train = np.zeros(((x1, x2, x2)))\n",
    "print(similarity_matrix_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm2 by row\n",
    "norm2 = lambda X : K.expand_dims(K.sqrt(K.sum(X ** 2, 1)))\n",
    "# Cosine distance by its best definition#\n",
    "cosine = lambda X, Y : K.dot(X, K.transpose(Y))/norm2(X)/K.transpose(norm2(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity matrix for training data\n",
    "train_sim = np.ndarray(shape=(x1, x2, x2), dtype=float)\n",
    "for i in tqdm(range(x1)):\n",
    "    emb1 = embeddings[X_train['left'][i]]\n",
    "    emb2 = embeddings[X_train['right'][i]]\n",
    "    emb1 = tf.convert_to_tensor(emb1.astype(float), np.float64)\n",
    "    emb2 = tf.convert_to_tensor(emb2.astype(float), np.float64)\n",
    "    x = cosine(emb1, emb2)\n",
    "    with tf.Session():\n",
    "        train_sim[i] = x.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3 = X_test['left'].shape[0]\n",
    "x4 = X_test['left'].shape[1]\n",
    "test_sim = np.ndarray(shape=(x3, x4, x4), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity matrix for testing data\n",
    "for i in tqdm(range(x3)):\n",
    "    emb1 = embeddings[X_test['left'][i]]\n",
    "    emb2 = embeddings[X_test['right'][i]]\n",
    "    emb1 = tf.convert_to_tensor(emb1.astype(float), np.float64)\n",
    "    emb2 = tf.convert_to_tensor(emb2.astype(float), np.float64)\n",
    "    x = cosine(emb1, emb2)\n",
    "    with tf.Session():\n",
    "        test_sim[i] = x.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing all the matrices in hdf5 files\n",
    "hf = h5py.File('../objects/data.h5', 'w')\n",
    "hf.create_dataset('X_train', data=train_sim)\n",
    "hf.create_dataset('X_test', data=test_sim)\n",
    "hf.create_dataset('Y_test', data=Y_test)\n",
    "hf.create_dataset('Y_train', data=Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
